# ğŸ§  Retrieval-Augmented Generation (RAG) Using PocketFlow  
**Zero LangChain â€¢ Explicit Orchestration â€¢ Gemini + FAISS**

---

## ğŸ“Œ Overview

This repository implements a **complete Retrieval-Augmented Generation (RAG) system** using a **custom PocketFlow orchestration engine**.  
The system loads a PDF, builds a vector index, retrieves relevant context, and generates grounded answers using **Google Gemini** â€” **without LangChain or hidden abstractions**.

Everything is explicit, debuggable, and framework-free.

---

## ğŸš€ What This Project Does

- Loads a PDF document
- Extracts raw text from pages
- Splits text into overlapping chunks
- Generates vector embeddings
- Stores embeddings in **FAISS**
- Accepts user questions via terminal
- Retrieves relevant document chunks
- Builds a grounded prompt
- Generates answers using **Gemini**
- Loops until the user exits

---

## ğŸ› ï¸ Tech Stack

### Core Libraries
- **Python**
- **FAISS (faiss-cpu)** â€” vector similarity search
- **pypdf** â€” PDF text extraction
- **sentence-transformers** â€” embedding generation
- **google-generativeai** â€” Gemini LLM access

### Architecture
- **PocketFlow (custom implementation)** â€” workflow orchestration
- **No LangChain**
- **No implicit pipelines**
- **Explicit data flow**

---

## ğŸ§  Why PocketFlow?

PocketFlow is used instead of LangChain to achieve:

- Full control over execution
- Explicit step-by-step logic
- Conditional routing
- Easy debugging
- Interview- and production-friendly architecture

This project demonstrates **how RAG works internally**, not just how to call a framework.

---

## ğŸ§© High-Level Architecture

User Input
â†“
FAISS Retriever
â†“
Prompt Builder
â†“
Gemini LLM
â†“
Answer Display
â†“
Repeat


Input
  â†’ Retrieve
      â†’ Prompt
          â†’ Gemini
              â†’ Display
                  â†’ Input

---

## ğŸ” API Key Setup (Google Colab)

- Open **Google Colab â†’ Settings â†’ Secrets**
- Add the following secret:

```text
RAGAGENTKEY = YOUR_GEMINI_API_KEY

